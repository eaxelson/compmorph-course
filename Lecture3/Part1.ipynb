{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTATIONAL MORPHOLOGY WITH HFST TOOLS - LECTURE 3\n",
    "\n",
    "* (1.) Disambiguation\n",
    "* (2.) Probabilities, basics\n",
    "* (3.) Back to disambiguation\n",
    "* (4.) Spelling correction\n",
    "* (5.) Logprobs\n",
    "* (6.) Summary of types of finite-state automata and transducers\n",
    "\n",
    "## 1. Disambiguation\n",
    "\n",
    "Some Finnish noun examples:\n",
    "\n",
    "* nainen\n",
    "* lautasilla\n",
    "* lautasilta\n",
    "* poikasilla\n",
    "* poikasilta\n",
    "\n",
    "The Finnish noun examples with analyses:\n",
    "\n",
    "* nainen ü°í nainen +N +Sg +Nom (‚Äúwoman‚Äù)\n",
    "* lautasilla ü°í lautanen +N +Pl +Ade (‚Äúon plates‚Äù)\n",
    "* lautasilta ü°í lautanen +N +Pl +Abl (‚Äúfrom plates‚Äù)\n",
    "* poikasilla ü°í poikanen +N +Pl +Ade (‚Äúwith cubs‚Äù)\n",
    "* poikasilta ü°í poikanen +N +Pl +Abl (‚Äúfrom cubs‚Äù)\n",
    "\n",
    "The Finnish noun examples with more analyses:\n",
    "\n",
    "* nainen ü°í naida +V +Pot +Pres +Sg1 (‚Äúit seems I‚Äôll marry‚Äù)\n",
    "* lautasilla ü°í lauta#silla +N +Sg +Ade (‚Äúboard rayon‚Äù)\n",
    "* lautasilta:\n",
    "  - ü°í lauta#silta +N +Sg +Nom (‚Äúboard bridge‚Äù)\n",
    "  - ü°í lautas#ilta +N +Sg +Nom (‚Äúplate evening‚Äù)\n",
    "*  poikasilla ü°í poika#silla +N +Sg +Ade (‚Äúboy rayon‚Äù)\n",
    "*  poikasilta:\n",
    "  - ü°í poika#silta +N +Sg +Nom (‚Äúboy bridge‚Äù)\n",
    "  - ü°í poikas#ilta +N +Sg +Nom (‚Äúcub evening‚Äù)\n",
    "\n",
    "How disambiguate?\n",
    "\n",
    "* We could disambiguate (= find one unambiguous analysis) by looking at the word in context.\n",
    "* However, if we don‚Äôt have any context, we may still have a sense of which analyses are more likely _a priori_.\n",
    "* A priori = in general, without further information.\n",
    "* _A posteriori_, when we have more information, it may turn out that the most likely analysis a priori is not the correct one, but it is the best guess without more information.\n",
    "\n",
    "A priori assumptions:\n",
    "\n",
    "* \"Nainen +N\" is more common than \"naida +V\".\n",
    "* Singular (+Sg) is more common than Plural (+Pl).\n",
    "* Nominative (+Nom) is more common than the other cases.\n",
    "* Adessive (+Ade) is slightly more common than Ablative case (+Abl).\n",
    "* Single-stem words are more common than compound words.\n",
    "\n",
    "Model with probabilities\n",
    "\n",
    "<img src=\"img/model_with_probabilities.png\">\n",
    "\n",
    "## 2. Probabilities, basics\n",
    "\n",
    "What is probability?\n",
    "\n",
    "* Probability is the measure of the likelihood that an event will occur.\n",
    "* Probability is quantified as a number between 0 and 1\n",
    "  * 0 indicates impossibility\n",
    "  * 1 indicates certainty\n",
    "\n",
    "Objective probability\n",
    "\n",
    "* The most popular version of objective probability is frequentist probability.\n",
    "* Claims that the probability denotes the relative frequency of occurrence of an experiment's outcome.\n",
    "* The experiment is repeated many times.\n",
    "* This interpretation considers probability to be the relative frequency \"in the long run\" of outcomes.\n",
    "* Typical examples:\n",
    "  * throwing a dice (1, 2, 3, 4, 5, 6)\n",
    "  * throwing a coin (\"heads or tails\")\n",
    "\n",
    "Discrete probability distribution of outcome from throwing an unbiased six-sided die\n",
    "\n",
    "<img src=\"img/one_six_sided_die.png\">\n",
    "\n",
    "Probability of mutually exclusive events ü°í ADD probabilities together\n",
    "\n",
    "```\n",
    "P(S = 1 or S = 2) =\n",
    "P(S = 1) + P (S = 2) =\n",
    "1/6 + 1/6 =\n",
    "2/6 =\n",
    "1/3\n",
    "```\n",
    "\n",
    "Probability of all possible events combined ü°í The sum must be 1!\n",
    "\n",
    "```\n",
    "P(S = 1 or S = 2  or S = 3 or\n",
    "  S = 4  or S = 5  or S = 6) =\n",
    "P(S = 1) + P (S = 2) + P (S = 3) +\n",
    "  P(S = 4) + P (S = 5) + P (S = 6) =\n",
    "1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6 =\n",
    "6/6 =\n",
    "1\n",
    "```\n",
    "\n",
    "Discrete probability distribution of outcome from throwing two unbiased six-sided dice\n",
    "\n",
    "<img src=\"img/two_six_sided_dice.png\">\n",
    "\n",
    "Probability of independent events that co-occur ü°í MULTIPLY probabilities together\n",
    "\n",
    "```\n",
    "P(Sblack = 1 and Swhite = 1) =\n",
    "P(Sblack = 1) * P(Swhite = 1) =\n",
    "1/6 * 1/6 =\n",
    "1/36\n",
    "```\n",
    "\n",
    "Probability of mutually exclusive events ü°í Involves both addition and multiplication\n",
    "\n",
    "```\n",
    "P((Sblack = 5 and Swhite = 6) or (Sblack = 6 and Swhite = 5)) =\n",
    "P(Sblack = 5) * P(Swhite = 6) + P(Sblack = 6) * P(Swhite = 5) =\n",
    "1/6 * 1/6 + 1/6 * 1/6 =\n",
    "1/36 + 1/ 36 =\n",
    "2/36 =\n",
    "1/18\n",
    "```\n",
    "\n",
    "For comparison: A continuous probability distribution\n",
    "\n",
    "<img src=\"img/continuous_probability_distribution.png\">\n",
    "\n",
    "<i>Image from Wikipedia.</i>\n",
    "\n",
    "* The variables are here real-valued (\"floats\") rather than discrete categories (\"ints\").\n",
    "* We get a smooth curve for the probability distribution, such as this Gaussian curve, the so-called normal distribution.\n",
    "\n",
    "Subjective probability\n",
    "\n",
    "* The most popular version of subjective probability is Bayesian probability.\n",
    "* Rather than relative frequency in a series of experiments, subjectivists think of probabilities as degrees of belief.\n",
    "* \"The price at which you would buy or sell a bet that pays 1 unit of utility (such as money) if an event occurs, and 0 if the event does not occur.\"\n",
    "* Examples:\n",
    "  * Is there life on Mars?\n",
    "  * Will Trump win the presidential election in the USA? (Now, we know he did.)\n",
    "  * Will it snow in Helsinki tomorrow?\n",
    "  * Given specific symptoms, does a patient have cancer?\n",
    "\n",
    "Prior and posterior probabilities\n",
    "\n",
    "* In Bayesian statistics, there are prior probabilities and posterior probabilities\n",
    "  * also called a priori and a posteriori probabilities\n",
    "* The prior probability states the general assumptions made in the model, such as:\n",
    "  * P(\"It snows in Helsinki in January\")\n",
    "* The posterior probability is a product of two probabilities: the prior probability and a conditional probability, such as:\n",
    "  * P(\"It snows in Helsinki on 23 Jan 2018\" | \"It snows in Helsinki in January\") * P(\"It snows in Helsinki in January\")\n",
    "\n",
    "## 3. Back to disambiguation\n",
    "\n",
    "The \"poikasilla\" ambiguity: 1) poikanen +N +Pl +Ade\n",
    "\n",
    "<img src=\"img/poikanen_n_pl_ade.png\">\n",
    "\n",
    "Do the calculations\n",
    "\n",
    "```\n",
    "Prob(poikanen +N +Pl +Ade)\n",
    "= Prob(+N) * Prob(\"poikanen\") * Prob(+Pl) * Prob(+Ade)\n",
    "= 0.5 * 0.0001 * 0.3 * 0.05\n",
    "= 0.00000075\n",
    "= 7.5 * 10^-7\n",
    "```\n",
    "\n",
    "The \"poikasilla\" ambiguity: 2) poika#silla +N +Sg +Nom\n",
    "\n",
    "<img src=\"img/poika_silla_n_sg_nom.png\">\n",
    "\n",
    "Do the calculations:\n",
    "\n",
    "```\n",
    "Prob(poika#silla +N +Sg +Nom)\n",
    "= Prob(+N) * Prob(‚Äúpoika‚Äù) * Prob(Compound word) * Prob(‚Äúsilla‚Äù) * Prob(+Sg) * Prob(+Nom)\n",
    "= 0.5 * 0.001 * 0.1 * 0.000001 * 0.6 * 0.55\n",
    "= 0.0000000000165\n",
    "= 1.65 * 10^-11\n",
    "```\n",
    "\n",
    "Compare and pick the more likely alternative ü°í The analysis `poikanen +N +Pl +Ade` is almost 50000 times more likely\n",
    "than `poika#silla +N +Sg +Nom` (in this invented model).\n",
    "\n",
    "Formulated in lexc format with weights (The syntax is correct, but don't do it exactly like this yet):\n",
    "\n",
    "```\n",
    "Multichar_Symbols +N +Sg +Pl +Nom +Ade +Abl ^A ^I ^J ^K ^S ^T\n",
    "\n",
    "LEXICON Root\n",
    "Nouns \"weight: 0.5\" ;\n",
    "Verbs \"weight: 0.3\" ;\n",
    "...\n",
    "\n",
    "LEXICON Nouns\n",
    "ilta:il^Ta          Number \"weight: 0.0002\" ;\n",
    "lauta:lau^Ta        Number \"weight: 0.00001\" ;\n",
    "lautanen:lauta^S    Number \"weight: 0.0001\" ;\n",
    "nainen:nai^S        Number \"weight: 0.001\" ;\n",
    "poika:po^J^Ka       Number \"weight: 0.001\" ;\n",
    "poikanen:poika^S    Number \"weight: 0.0001\" ;\n",
    "silla:silla         Number \"weight: 0.000001\" ;\n",
    "silta:sil^Ta        Number \"weight: 0.0005\" ;\n",
    "...\n",
    "\n",
    "LEXICON Number\n",
    "+Sg:0               Case \"weight: 0.6\" ;\n",
    "+Pl:^I              Case \"weight: 0.3\" ;\n",
    "#:0  Nouns \"weight: 0.1\" ; ! Back to collect more stems\n",
    "\n",
    "LEXICON Case\n",
    "+Nom:0              # \"weight: 0.55\" ;\n",
    "+Ade:ll^A           # \"weight: 0.05\" ;\n",
    "...\n",
    "```\n",
    "\n",
    "## 4. Spelling correction\n",
    "\n",
    "### Virtual keyboard of a mobile device\n",
    "\n",
    "The D key was \"pressed\" (that is, touched).\n",
    "\n",
    "<img src=\"img/d_pressed.png\">\n",
    "\n",
    "### Noisy virtual keyboard of a mobile device\n",
    "\n",
    "The D key was \"pressed\" (that is, touched), but the intended key was actually F!\n",
    "\n",
    "<img src=\"img/f_intended.png\">\n",
    "\n",
    "Assume probabilities:\n",
    "\n",
    "```\n",
    "P(p = F | i = F) = 0.7\n",
    "P(p = D | i = F) = 0.1\n",
    "P(p = G | i = F) = 0.1\n",
    "P(p = R | i = F) = 0.025\n",
    "P(p = T | i = F) = 0.025\n",
    "P(p = C | i = F) = 0.025\n",
    "P(p = X | i = F) = 0.0125\n",
    "P(p = V | i = F) = 0.0125\n",
    "```\n",
    "\n",
    "For instance, read the last line as: \"The Probability that V was\n",
    "pressed when actually F was the intended key  is 0.0125.\"\n",
    "\n",
    "Create an FST that generates \"noise\" (errors) and invert it to get a spell checker\n",
    "\n",
    "<img src=\"img/spell_checker_model.png\">\n",
    "\n",
    "```\n",
    "Noisy surface 1:  poikasilla   (no error)\n",
    "Noisy surface 2:  loikasilla   (substitution)\n",
    "Noisy surface 3:  poikaslla    (deletion)\n",
    "Noisy surface 4:  ppoikasilla  (insertion)\n",
    "Noisy surface 5:  opikasilla   (transposition)\n",
    "... etc\n",
    "```\n",
    "\n",
    "xfst script snippet for a spell checker\n",
    "\n",
    "```\n",
    "! Use the .l operator to project only lower level (= surface forms) of the\n",
    "! transducer; we are not interested in the upper level (= lexical forms)\n",
    "define Vocabulary [ Lexicon .o. AlternationRules ].l ;\n",
    "\n",
    "! Add \"noise\" (spelling errors); replace rules are optional when in brackets ()\n",
    "define Substitution  [ f (->) d::0.1 ] .o. [ f (->) g::0.1 ] .o.\n",
    "                     [ f (->) r::0.025 ] .o. [ f (->) t::0.025 ] .o.\n",
    "                     etc ... ;\n",
    "\n",
    "! Define a transducer from correctly spelled words to words containing errors\n",
    "define NoisyVocabulary  Vocabulary .o. Substitution ;\n",
    "\n",
    "! Use the .i operator to invert the transducer, such that the input is noisy\n",
    "! words and the output is correctly spelled words\n",
    "define SpellChecker  [ NoisyVocabulary ].i ;\n",
    "\n",
    "! The spell checker is ready to use\n",
    "regex SpellChecker ;\n",
    "\n",
    "```\n",
    "\n",
    "Again, the syntax is correct, but there is something left to fix with the weights...\n",
    "\n",
    "## 5. Logprobs\n",
    "\n",
    "Back to the probabilities\n",
    "\n",
    "We had the probability:\n",
    "\n",
    "```\n",
    "Prob(poika#silla +N +Sg +Nom)\n",
    "= Prob(+N) * Prob(\"poika\") * Prob(Compound word) * Prob(\"silla\") * Prob(+Sg) * Prob(+Nom)\n",
    "= 0.5 * 0.001 * 0.1 * 0.000001 * 0.6 * 0.55\n",
    "= 0.0000000000165\n",
    "= 1.65 * 10^-11    \n",
    "```\n",
    "\n",
    "This product of probabilities can be written as:\n",
    "\n",
    "```\n",
    "This product of probabilities can be written as:\n",
    "(5 * 10^-1) * 10^-3 * 10^-1 * 10^-6 * (6 * 10^-1) * (5.5 * 10^-1)\n",
    "= 10^-0.301 * 10^-3 * 10^-1 * 10^-6 * 10^-0.222 * 10^-0.260\n",
    "= 10^-(0.301 + 3 + 1 + 6 + 0.222 + 0.260) =\n",
    "= 10^-10.783\n",
    "= 0.0000000000165\n",
    "```\n",
    "\n",
    "If we agree on some base, such as 10, then instead of multiplying actual probabilities\n",
    "of co-occuring indendent events, we can add the (negative) exponents of the probabilities,\n",
    "which is faster and more manageable.\n",
    "\n",
    "Instead of `0.5 * 0.001 * 0.1 * 0.000001 * 0.6 * 0.55 = 0.0000000000165`,\n",
    "we get: `0.301 + 3 + 1 + 6 + 0.222 + 0.260 = 10.783`\n",
    "\n",
    "What we are doing is taking the negative logarithm of the probabilities:\n",
    "`10^-10.783 = 0.0000000000165` ü°í `-log10 0.0000000000165 = 10.783`\n",
    "\n",
    "* A negative logarithm of a probability is called a _logprob_.\n",
    "* A logprob can be seen as a penalty term or a cost: \"if you do this operation you'll have to pay this much.\"\n",
    "* Logprobs add up from operations performed in a sequence. \n",
    "* If the probability of some operation is 1, that is, there is only one possible outcome:\n",
    "  * The logprob is `-log10 1 = 0`\n",
    "  * That is, there is no penalty if there is only one certain outcome.\n",
    "  * This makes sense.\n",
    "\n",
    "### Weights in HFST\n",
    "\n",
    "* HFST does not really support probabilities as such.\n",
    "* HFST supports additive weights, such as logprobs.\n",
    "* Weights in lexc could look like:\n",
    "\n",
    "```\n",
    "LEXICON Nouns\n",
    "ilta:il^Ta          Number \"weight: 3.69897\" ;\n",
    "lauta:lau^Ta        Number \"weight: 5.00000\" ;\n",
    "...\n",
    "```\n",
    "\n",
    "* Weights in xfst rules could look like:\n",
    "\n",
    "```\n",
    "[ f (->) g::1.000 ] .o. [ f (->) r::1.602 ]\n",
    "```\n",
    "\n",
    "### Lexc with weights revisited\n",
    "\n",
    "<img src=\"img/lexc_with_weights_revisited.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hfst_dev import compile_lexc_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = compile_lexc_script(\"\"\"\n",
    "Multichar_Symbols +N +Sg +Pl +Nom +Ade +Abl ^A ^I ^J ^K ^S ^T \n",
    "\n",
    "LEXICON Root\n",
    "           Nouns \"weight: 0.30103\" ;   ! Probability: 0.5\n",
    "           Verbs \"weight: 0.52288\" ;   ! Probability: 0.3\n",
    "\n",
    "LEXICON Nouns\n",
    "ilta:il^Ta          Number \"weight: 3.69897\" ; ! Probability: 0.0002\n",
    "lauta:lau^Ta        Number \"weight: 5.00000\" ; ! Probability: 0.00001\n",
    "lautanen:lauta^S    Number \"weight: 4.00000\" ; ! Probability: 0.0001\n",
    "nainen:nai^S        Number \"weight: 3.00000\" ; ! Probability: 0.001\n",
    "poika:po^J^Ka       Number \"weight: 3.00000\" ; ! Probability: 0.001\n",
    "poikanen:poika^S    Number \"weight: 4.00000\" ; ! Probability: 0.0001\n",
    "silla:silla         Number \"weight: 6.00000\" ; ! Probability: 0.000001\n",
    "silta:sil^Ta        Number \"weight: 3.30103\" ; ! Probability: 0.0005\n",
    "\n",
    "LEXICON Number\n",
    "+Sg:0               Case \"weight: 0.22185\" ;  ! Probability: 0.6\n",
    "+Pl:^I              Case \"weight: 0.52288\" ;  ! Probability: 0.3\n",
    "#:0  Nouns \"weight: 1.00000\" ; ! Back to collect more stems\n",
    "                                              ! Probability: 0.1\n",
    "LEXICON Case\n",
    "+Nom:0              # \"weight: 0.259637\" ;    ! Probability: 0.55\n",
    "+Ade:ll^A           # \"weight: 1.301030\" ;    ! Probability: 0.05\n",
    "\n",
    "END\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invert and test the transducer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.invert()\n",
    "tr.minimize()\n",
    "print(tr.lookup('poika^S^Ill^A'))\n",
    "print(tr.lookup('poika^Sil^Ta'))\n",
    "print(tr.lookup('po^J^Kasil^Ta'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisited xfst script for a spell checker\n",
    "\n",
    "<img src=\"img/xfst_with_weights_revisited.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hfst_dev import compile_xfst_script\n",
    "compile_xfst_script(\"\"\"\n",
    "! Toy example\n",
    "define Lexicon {for}|{fight}|{right}|{tight}|{of}|{or} ;\n",
    "define AlternationRules ?* ;\n",
    "\n",
    "! Use the .l operator to project only lower level (= surface forms) of the \n",
    "! transducer; we are not interested in the upper level (= lexical forms)\n",
    "define Vocabulary [ Lexicon .o. AlternationRules ].l ;\n",
    "\n",
    "! Add \"noise\" (spelling errors); replace rules are optional when in brackets ()\n",
    "define Substitution  [ f (->) d::1.000 ] .o. [ f (->) g::1.000 ] .o. \n",
    "                     [ f (->) r::1.602 ] .o. [ f (->) t::1.602 ] ; \n",
    "!                                                                .o. etc ... ;\n",
    "\n",
    "! Define a transducer from correctly spelled words to words containing errors\n",
    "define NoisyVocabulary  Vocabulary .o. Substitution ;\n",
    "\n",
    "! Use the .i operator to invert the transducer, such that the input is noisy\n",
    "! words and the output is correctly spelled words\n",
    "define SpellChecker  [ NoisyVocabulary ].i ;\n",
    "\n",
    "! The spell checker is ready to use\n",
    "regex SpellChecker ;\n",
    "invert net\n",
    "minimize net\n",
    "set print-weight ON\n",
    "\n",
    "apply up right\n",
    "echo --\n",
    "apply up or\n",
    "echo --\n",
    "apply up for\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary of types of finite-state automata and transducers\n",
    "\n",
    "### Finite-state automaton (FSA)\n",
    "\n",
    "A finite-state automaton (FSA) - or finite automaton - is a network consisting of nodes,\n",
    "which represent states, and directed arcs connecting the states,\n",
    "which represent transitions between states.\n",
    "Every arc is labeled with a symbol that is consumed from input.\n",
    "State transitions can also take place without consuming any input;\n",
    "these transitions are called epsilon transitions.\n",
    "\n",
    "<img src=\"img/fsa.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: use graphviz package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HfstIterableTransducer is a special class for generating transducers\n",
    "# from scratch or iterating them state by state and transition by transition.\n",
    "# It does not support most of the ordinary transducer functions.\n",
    "from hfst_dev import HfstIterableTransducer, EPSILON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = HfstIterableTransducer()\n",
    "tr.add_transition(0, 1, EPSILON, EPSILON, 0)\n",
    "tr.add_transition(0, 2, EPSILON, EPSILON, 0)\n",
    "tr.add_transition(1, 3, 'b', 'b', 0)\n",
    "tr.add_transition(2, 1, EPSILON, EPSILON, 0)\n",
    "tr.add_transition(2, 2, 'b', 'b', 0)\n",
    "tr.add_transition(3, 3, 'b', 'b', 0)\n",
    "tr.add_transition(3, 4, EPSILON, EPSILON, 0)\n",
    "tr.add_transition(3, 4, 'c', 'c', 0)\n",
    "tr.set_final_weight(4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the transducer in AT&T format:\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted finite-state automaton (WFSA)\n",
    "\n",
    "A weighted finite-state automaton (WFSA) is an FSA with weights on the arcs.\n",
    "Weights make it possible to find the best way ‚Äúthrough\" an automaton for the given input;\n",
    "you want to take the path with the lowest weight.\n",
    "\n",
    "<img src=\"img/wfsa.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = HfstIterableTransducer()\n",
    "tr.add_transition(0, 1, 'a', 'a', 2.7002)\n",
    "tr.add_transition(0, 2, 'b', 'b', 1.0)\n",
    "tr.add_transition(1, 1, 'a', 'a', 1.0)\n",
    "tr.add_transition(1, 2, 'b', 'b', 0)\n",
    "tr.add_transition(2, 2, 'b', 'b', 0.59961)\n",
    "tr.add_transition(2, 3, 'c', 'c', 0)\n",
    "tr.set_final_weight(2, 0)\n",
    "tr.set_final_weight(3, 1.0)\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted automata can be used to decide between two alternatives.\n",
    "For example, you‚Äôre running a speech recognition system and the user says \"I have to go.\"\n",
    "How do you know the user didn‚Äôt say, \"I have two go\"?\n",
    "First, you come up with a probability of words occurring next to each other\n",
    "(for example, P(\"to go\") and P(\"two go\")) - a language model.\n",
    "Then, you translate those probabilities into weights for your finite state machine.\n",
    "Then, when you‚Äôre deciding between \"to\" and \"two,\" you pick the sentence with lower weight (\"to\").\n",
    "\n",
    "### Finite-state transducer (FST)\n",
    "\n",
    "A finite-state transducer (FST) is a finite automaton for which\n",
    "each transition has an input label and an output label.\n",
    "\n",
    "<img src=\"img/fst.png\">\n",
    "\n",
    "It recognizes whether the two strings are valid correspondences (or translations)\n",
    "of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = HfstIterableTransducer()\n",
    "tr.add_transition(0, 1, 'a', 'a', 0)\n",
    "tr.add_transition(1, 2, 'b', 'b', 0)\n",
    "tr.add_transition(1, 3, 'a', 'a', 0)\n",
    "tr.add_transition(2, 1, 'b', 'c', 0)\n",
    "tr.add_transition(2, 3, 'a', 'a', 0)\n",
    "tr.set_final_weight(3, 0)\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted finite-state transducer (WFST)\n",
    "\n",
    "A weighted finite-state transducer (WFST) is a finite automaton for which\n",
    "each transition has an input label, an output label, and a weight.\n",
    "\n",
    "<img src=\"img/wfst.png\">\n",
    "\n",
    "The initial state is labeled 0. The final state is 2 with final weight of 3.5.\n",
    "Any state with non-infinite final weight is a final state. There is a transition\n",
    "from state 0 to 1 with input label a, output label x, and weight 0.5.\n",
    "This machine transduces, for instance, the string \"ac\" to \"xz\" with weight 6.5\n",
    "(the sum of the arc and final weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = HfstIterableTransducer()\n",
    "tr.add_transition(0, 1, 'a', 'x', 0.5)\n",
    "tr.add_transition(0, 1, 'b', 'y', 1.5)\n",
    "tr.add_transition(1, 2, 'c', 'z', 2.5)\n",
    "tr.set_final_weight(2, 3.5)\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HfstTransducer before lookup.\n",
    "from hfst_dev import HfstTransducer\n",
    "TR = HfstTransducer(tr)\n",
    "print(TR.lookup('ac'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "* The Beesley & Karttunen book does not cover weighted finite-state machines. Weights were fairly new at about the time when the book was published in 2003.\n",
    "* HFST: [Using weights](https://github.com/hfst/python-hfst-4.0/wiki/Weights)\n",
    "* HFST [ospell](https://github.com/hfst/hfst-ospell/wiki)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
