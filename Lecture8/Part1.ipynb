{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTATIONAL MORPHOLOGY WITH HFST TOOLS - LECTURE 8\n",
    "\n",
    "* (1.) Optimizing unweighted finite-state networks\n",
    "* (2.) Optimizing weighted finite-state networks\n",
    "\n",
    "## 1. Optimizing unweighted finite-state networks\n",
    "\n",
    "### 1.1. Example lexicon\n",
    "\n",
    "Let’s first create a noun lexicon and add word stems to it.\n",
    "\n",
    "<img src=\"img/noun_lexicon.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hfst_dev import HfstIterableTransducer, EPSILON\n",
    "# This will be the entire lexicon\n",
    "lexicon = HfstIterableTransducer()\n",
    "add = lexicon.add_transition # shorter notation for adding transition\n",
    "# define sublexicon start and end states\n",
    "start_state = 1\n",
    "end_state = 8\n",
    "# and use consecutive numbering for states that will be added\n",
    "# (remember to skip end state)\n",
    "state = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could add lexemes manually, e.g.\n",
    "#   add(2, 3, 'k', 'k', 0.0)\n",
    "#   add(3, 4, 'i', 'i', 0.0)\n",
    "#   add(4, 5, 's', 's', 0.0)\n",
    "#   add(5, 6, 'k', 'k', 0.0)\n",
    "#   add(6, 7, 'o', 'o', 0.0)\n",
    "# but it is easier this way:\n",
    "add(start_state, state, EPSILON, EPSILON, 0.0) # from start state to beginning of lexeme\n",
    "for symbol in list('kisko'):\n",
    "    add(state, state+1, symbol, symbol, 0.0)\n",
    "    state += 1\n",
    "add(state, end_state, EPSILON, EPSILON, 0.0) # from end of lexeme to end state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip end state\n",
    "assert(state == 7)\n",
    "state += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rest of the lexemes\n",
    "for lexeme in ('kissa','koira','kori','koulu','taulu','tori','tuoksu'):\n",
    "    add(start_state, state, EPSILON, EPSILON, 0.0)\n",
    "    for symbol in list(lexeme):\n",
    "        add(state, state+1, symbol, symbol, 0.0)\n",
    "        state += 1\n",
    "    add(state, end_state, EPSILON, EPSILON, 0.0)\n",
    "    state += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that the result is as intended\n",
    "test_lexicon = HfstIterableTransducer(lexicon)\n",
    "test_lexicon.add_transition(0, 1, EPSILON, EPSILON, 0.0)\n",
    "test_lexicon.set_final_weight(8, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hfst_dev import HfstTransducer, regex\n",
    "tr = HfstTransducer(test_lexicon)\n",
    "tr.minimize()\n",
    "result = regex('{kisko}|{kissa}|{koira}|{kori}|{koulu}|{taulu}|{tori}|{tuoksu}')\n",
    "assert(result.compare(tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let’s create a continuation lexicon with case endings and start populating it.\n",
    "\n",
    "<img src=\"img/continuation_lexicon.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(state == 50)\n",
    "state += 1\n",
    "start_state = 50\n",
    "end_state = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add case endings\n",
    "for ending in ('a','lla','lle','lta','n'):\n",
    "    # skip end state\n",
    "    if state == end_state:\n",
    "        state += 1\n",
    "    add(start_state, state, EPSILON, EPSILON, 0.0)\n",
    "    for symbol in list(ending):\n",
    "        add(state, state+1, symbol, symbol, 0.0)\n",
    "        state += 1\n",
    "    add(state, end_state, EPSILON, EPSILON, 0.0)\n",
    "    state += 1\n",
    "# make case ending optional\n",
    "add(50, 53, EPSILON, EPSILON, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(state == 68)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we tie the lexicons together and also add an epsilon transition from the end of the stem lexicon to its beginning in order to allow compound words\n",
    "\n",
    "<img src=\"img/compound_lexicon.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add(8, 50, EPSILON, EPSILON, 0.0)\n",
    "add(8, 1, EPSILON, EPSILON, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that the result is as intended\n",
    "test_lexicon = HfstIterableTransducer(lexicon)\n",
    "test_lexicon.add_transition(0, 1, EPSILON, EPSILON, 0.0)\n",
    "test_lexicon.set_final_weight(53, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = HfstTransducer(test_lexicon)\n",
    "tr.minimize()\n",
    "result = regex('[{kisko}|{kissa}|{koira}|{kori}|{koulu}|{taulu}|{tori}|{tuoksu}]+ ({a}|{lla}|{lle}|{lta}|{n})')\n",
    "assert(result.compare(tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let’s add a lexicon for verb stems.\n",
    "\n",
    "<img src=\"img/lexicon_verb_stems.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(state == 68)\n",
    "start_state = 68\n",
    "end_state = 75\n",
    "state += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add verb stem endings\n",
    "for stem in ('kisko','tuoksu'):\n",
    "    # skip end state\n",
    "    if state == end_state:\n",
    "        state += 1\n",
    "    add(start_state, state, EPSILON, EPSILON, 0.0)\n",
    "    for symbol in list(stem):\n",
    "        add(state, state+1, symbol, symbol, 0.0)\n",
    "        state += 1\n",
    "    add(state, end_state, EPSILON, EPSILON, 0.0)\n",
    "    state += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a continuation lexicon for present-tense person endings (mainly)\n",
    "\n",
    "<img src=\"img/lexicon_person_endings.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(state == 83)\n",
    "start_state = 83\n",
    "end_state = 86\n",
    "state +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add person endings\n",
    "for ending in ('a','mme','n','t','tte','vat'):\n",
    "    # skip end state\n",
    "    if state == end_state:\n",
    "        state += 1\n",
    "    add(start_state, state, EPSILON, EPSILON, 0.0)\n",
    "    for symbol in list(ending):\n",
    "        add(state, state+1, symbol, symbol, 0.0)\n",
    "        state += 1\n",
    "    add(state, end_state, EPSILON, EPSILON, 0.0)\n",
    "    state += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make person ending optional\n",
    "add(83, 86, EPSILON, EPSILON, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(state == 103)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s tie the verb stem lexicon together with the endings lexicon.\n",
    "\n",
    "<img src=\"img/lexicon_verbs_and_endings.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add(75, 83, EPSILON, EPSILON, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and let’s tie the whole network together with a start state and end state\n",
    "\n",
    "<img src=\"img/lexicon_tied_together.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add(0, 1, EPSILON, EPSILON, 0.0)\n",
    "add(0, 68, EPSILON, EPSILON, 0.0)\n",
    "add(53, 103, EPSILON, EPSILON, 0.0)\n",
    "add(86, 103, EPSILON, EPSILON, 0.0)\n",
    "lexicon.set_final_weight(103, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that the result is as intended\n",
    "tr = HfstTransducer(lexicon)\n",
    "tr.minimize()\n",
    "result = regex(\"\"\"\n",
    "[ [{kisko}|{kissa}|{koira}|{kori}|{koulu}|{taulu}|{tori}|{tuoksu}]+ ({a}|{lla}|{lle}|{lta}|{n}) ]\n",
    "| \n",
    "[ [{kisko}|{tuoksu}] ({a}|{mme}|{n}|{t}|{tte}|{vat}) ]\n",
    "\"\"\")\n",
    "assert(result.compare(tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. The resulting network\n",
    "\n",
    "The network is now ready. It has some advantages\n",
    "\n",
    "* The structure is logical\n",
    "* Building the network by adding words and endings to it (through the union operation) is simple and fast\n",
    "\n",
    "However, there are also some disadvantages\n",
    "\n",
    "* The automaton is non-deterministic and contains epsilon transitions\n",
    "  * This means that from a specific state, some symbol s can take you to more than one another state.\n",
    "  * For instance, from the initial state 0, the symbol “k” could take you to state 3, 10, 16, 22, 27, or 70.\n",
    "  * Imagine a scenario with a more realistic, larger vocabulary: using the network would be very slow, because of all the paths that have to be investigated.\n",
    "\n",
    "<img src=\"img/lexicon_epsilon_transitions.png\">\n",
    "\n",
    "### 1.3. Determinization of the network\n",
    "\n",
    "To start determinizing the network...\n",
    "\n",
    "* 1. We would merge the states 3, 10, 16, 22, 27, and 70 into one single, new state.\n",
    "* 2. We would create one transition with the symbol “k” from the initial state to our new state. (Not shown in the picture on the next page.)\n",
    "\n",
    "<img src=\"img/determinizing_the_network_1.png\">\n",
    "\n",
    "* 3. Then, from the new state, the symbol “o” takes us to the states 17, 23 or 28, so we would merge these states into one new state, too.\n",
    "\n",
    "<img src=\"img/determinizing_the_network_2.png\">\n",
    "\n",
    "* 4. And the symbol “i” takes us to the states 4, 11, and 71, so we keep merging states and updating the transitions.\n",
    "\n",
    "<img src=\"img/determinizing_the_network_3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = HfstTransducer(lexicon)\n",
    "tr.determinize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Minimization of the network\n",
    "\n",
    "Furthermore, there is another disadvantage with the original network\n",
    "\n",
    "* The network is unnecessarily large\n",
    "  * There are some “tails” that occur in many places that could be merged.\n",
    "  * For instance, the ends of the stems “kori” and “tori” are identical, as are the ends of the stems “koulu” and “taulu”.\n",
    "  * Determinization will not fix these issues, so we can use a separate minimization algorithm.\n",
    "\n",
    "<img src=\"img/minimizing_the_network_1.png\">\n",
    "\n",
    "<img src=\"img/minimizing_the_network_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = HfstTransducer(lexicon)\n",
    "tr.minimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Further issues\n",
    "\n",
    "* The epsilon transition back to the beginning that produces compound words is nasty:\n",
    "  * Full determinization may actually bloat the size of the network.\n",
    "  * Consider, for instance, if we had the stem “koulu” that can get an “a” appended for partitive (“koulua”), but in addition, “a” could be the beginning of a second stem, such as “aamiainen” (“kouluaamiainen”).\n",
    "  * Then, we would need one node in the network that is the starting point for all stems starting in “a” as the first stem in a word and another node with all the stems starting in “a” plus the endings starting in “a”.\n",
    "\n",
    "<img src=\"img/full_determinization.png\">\n",
    "\n",
    "* One might actually choose not to do a full determinization, but keep the epsilon transitions, for instance.\n",
    "\n",
    "#### Acceptors vs. transducers?\n",
    "\n",
    "* In the examples above, we have shown acceptors, with only an input symbol on the arcs\n",
    "  * Such as: `a b c d`\n",
    "* If we had a transducer, we would have pairs of symbols (`input:output`)\n",
    "  * Such as: `a:a a:b b:b c:e`\n",
    "* Determinization and minimization work the same in both cases.\n",
    "  * We just need to interpret the pairs of symbols as one single symbol, so `a:a` is another symbol than `a:b`.\n",
    "\n",
    "#### Algorithms\n",
    "\n",
    "* Determinization\n",
    "  * For instance: https://www.tutorialspoint.com/automata_theory/ndfa_to_dfa_conversion.htm\n",
    "* Minimization\n",
    "  * For instance: http://www.cs.engr.uky.edu/~lewis/essays/compilers/min-fa.html (todo: check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimizing weighted finite-state networks\n",
    "\n",
    "Optimizing weighted finite-state networks is basically the same as unweighted networks, but the weights may mess up things.\n",
    "We would like the optimized weighted network to produce the same weights as the unoptimized network.\n",
    "\n",
    "Assume the following probabilities:\n",
    "\n",
    "```\n",
    "Prob(Noun) = 0.5\n",
    "Prob(Verb) = 0.3\n",
    "Prob(tuoksu | Noun) = 0.0001\n",
    "Prob(Noun ending -a for partitive case) = 0.1\n",
    "Prob(tuoksu | Verb) = 0.001\n",
    "Prob(Verb ending -a for infinitive) = 0.05\n",
    "```\n",
    "\n",
    "Then we get the following probabilities for the full word forms “tuoksua”:\n",
    "\n",
    "* `Prob(tuoksua as a noun) = Prob(Noun) × Prob(tuoksu | Noun) × Prob(Noun ending -a for partitive case) = 0.5 × 0.0001 × 0.1 = 0.000005`\n",
    "* `Prob(tuoksua as a verb) = Prob(Verb) × Prob(tuoksu | Verb) × Prob(Verb ending -a for infinitive) = 0.3 × 0.001 × 0.05 = 0.000015`\n",
    "* `Prob(tuoksua as a noun or verb) = Prob(tuoksua as a noun) + Prob(tuoksua as a verb) =  0.000005 + 0.000015 = 0.00002`\n",
    "\n",
    "Shown as a network with probability weights:\n",
    "\n",
    "<img src=\"img/network_with_probability_weights.png\">\n",
    "\n",
    "However, usually weights are not probabilities as such.\n",
    "\n",
    "### 2.1. Semirings\n",
    "\n",
    "* Let’s replace the probabilities with some generic weights and replace the operators × and + with the generic semiring operators ⊗ and ⊕\n",
    "  * `Weight(tuoksua as a noun) = Weight(Noun) ⊗ Weight(tuoksu | Noun) ⊗ Weight(Noun ending -a for partitive case)`\n",
    "  * `Weight(tuoksua as a verb) = Weight(Verb) ⊗ Weight(tuoksu | Verb) ⊗ Weight(Verb ending -a for infinitive)`\n",
    "  * `Weight(tuoksua as a noun or verb) = Weight(tuoksua as a noun) ⊕ Weight(tuoksua as a verb)`\n",
    "\n",
    "### 2.2. Probability semiring\n",
    "\n",
    "* The weights should be interpreted as probabilties\n",
    "* The operator ⊗ should be interpreted as multiplication ×\n",
    "* The operator ⊕ should be interpreted as addition +\n",
    "* This is exactly what we have seen in our example already\n",
    "\n",
    "### 2.3. Log semiring\n",
    "\n",
    "* The weights should be interpreted as negative logprobs: for instance, – log Prob(tuoksu | Noun)\n",
    "* The operator ⊗ should be interpreted as addition +\n",
    "* The operator ⊕ should be interpreted as the rather complex operation: w1 ⊕ w2 = – log (10<sup>-w1</sup> + 10<sup>-w2</sup>)  (if we use 10 as our base; it can be something else, too)\n",
    "* Why? Because if w1 = – log<sub>10</sub> p1 and w2 = – log<sub>10</sub> p2 and p1 and p2 are probabilities, then w1 ⊕ w2 = – log<sub>10</sub>(10<sup>– log<sub>10</sub> p1</sup> + 10<sup>– log<sub>10</sub> p2</sup>) =  – log<sub>10</sub>(p1 + p2) (This is the logprob of the sum of two probabilities)\n",
    "\n",
    "Shown as a network with logprob weights in the log semiring\n",
    "\n",
    "<img src=\"img/network_with_logprob_weights.png\">\n",
    "\n",
    "### 2.4. Tropical semiring\n",
    "\n",
    "* The weights can still be interpreted as negative logprobs: for instance, –log(Prob(tuoksu | Noun))\n",
    "* The operator ⊗ should still be interpreted as addition +\n",
    "* The operator ⊕ is simplified and picks the minimum, that is, the path with lower overall weight: w1 ⊕ w2 = min(w1, w2) = { w1 if w1 < w2; w2 otherwise }\n",
    "\n",
    "Shown as a network with logprob weights in the tropical semiring:\n",
    "\n",
    "<img src=\"img/network_with_tropical_weights.png\">\n",
    "\n",
    "#### Another example of weighted determinization in the tropical semiring\n",
    "\n",
    "<img src=\"img/weighted_determinization_example.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hfst_dev import read_att_string\n",
    "tr = read_att_string(\n",
    "\"\"\"0 1 a a 0\n",
    "0 1 b b 1\n",
    "0 1 c c 4\n",
    "0 2 a a 3\n",
    "0 2 b b 4\n",
    "0 2 c c 7\n",
    "0 2 d d 0\n",
    "0 2 e e 1\n",
    "1 3 f f 1\n",
    "1 3 e e 0\n",
    "1 3 e e 2\n",
    "2 3 e e 10\n",
    "2 3 f f 11\n",
    "2 3 f f 13\n",
    "3 0\n",
    "\"\"\")\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.determinize()\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights must be pushed before minimization can take place:\n",
    "\n",
    "<img src=\"img/weight_pushing_and_minimization.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.minimize()\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other uses\n",
    "\n",
    "Mehryar Mohri did not work on morphology, but on automatic speech recognition:\n",
    "\n",
    "<img src=\"img/speech_recognition.png\">\n",
    "\n",
    "## Further reading\n",
    "\n",
    "* To learn more, you can read the full article by Mohri et al. at: http://www.cs.nyu.edu/~mohri/pub/csl01.pdf\n",
    "* There are more similar articles, such as the version that was actually published in Computer Speech and Language in 2002.\n",
    "* Or look at the OpenFST library: http://www.cs.columbia.edu/~mohri/"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
